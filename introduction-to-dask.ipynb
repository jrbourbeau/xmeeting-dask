{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `print('Hello Dask!')`\n",
    "## James Bourbeau\n",
    "#### WIPAC X-meeting\n",
    "October 9, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [What is / Why Dask?](#What-is-Dask?)\n",
    "\n",
    "- [High Level Collections](#High-Level-Collections)\n",
    "\n",
    "    - [Dask Arrays](#Dask-Arrays)\n",
    "    \n",
    "    - [Dask DataFrames](#Dask-DataFrames)\n",
    "\n",
    "- [Low Level Interface](#Low-Level-Interface)\n",
    "\n",
    "    - [Dask Delayed](#Dask-Delayed)\n",
    "\n",
    "\n",
    "- [Schedulers](#Schedulers)\n",
    "\n",
    "    - [Single Machine Schedulers](#Single-Machine-Schedulers)\n",
    "    \n",
    "    - [Distributed Scheduler](#Distributed-Scheduler)\n",
    "    \n",
    "- [Building on Dask](#Building-on-Dask)\n",
    "\n",
    "- [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Dask?\n",
    "\n",
    "- Dask is a flexible, open source library for parallel computing in Python\n",
    "\n",
    "    - GitHub: https://github.com/dask/dask\n",
    "    \n",
    "    - Documentation: https://docs.dask.org\n",
    "    \n",
    "- Two main components of Dask:\n",
    "\n",
    "    - High- and low-level interfaces for creating task graphs to perform a computation\n",
    "    \n",
    "    - Task schedulers to to execute the task graph\n",
    "    \n",
    "<img src=\"images/collections-schedulers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Dask?\n",
    "\n",
    "- Integrates well with the scientific Python ecosystem\n",
    "\n",
    "- Uses familiar APIs you're used to from NumPy, Pandas, and scikit-learn\n",
    "\n",
    "- Allows you to scale existing workflows with minimal rewriting\n",
    "\n",
    "- Dask works on your laptop, but also scales out to clusters\n",
    "\n",
    "- Offers great built-in diagnosic tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing task graphs\n",
    "\n",
    "#### High level interfaces:\n",
    "\n",
    "- [Bags](http://docs.dask.org/en/latest/bag.html): Parallel Python lists\n",
    "\n",
    "- [Arrays](http://docs.dask.org/en/latest/array.html): Parallel NumPy\n",
    "\n",
    "- [DataFrames](http://docs.dask.org/en/latest/dataframe.html): Parallel Pandas\n",
    "\n",
    "\n",
    "#### Low Level interfaces:\n",
    "\n",
    "- [Delayed](http://docs.dask.org/en/latest/delayed.html): Parallel function evaluation\n",
    "\n",
    "- [Futures](http://docs.dask.org/en/latest/futures.html): Real-time parallel function evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Collections\n",
    "\n",
    "[ [Back to top](#Outline) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dask Arrays\n",
    "\n",
    "- Dask arrays are a collection of NumPy ndarray arrays\n",
    "\n",
    "- Dask arrays implements a subset of the NumPy interface using blocked algorithms\n",
    "\n",
    "- For many purposes Dask arrays can serve as drop-in replacements for NumPy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/dask-array-black-text.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = np.arange(1, 50, 3)\n",
    "a_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da = da.arange(1, 50, 3, chunks=5)\n",
    "a_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask arrays are _lazily_ evaluated; the actual data in the array is not loaded until you ask for it.\n",
    "\n",
    "Similar to NumPy arrays, Dask arrays have a `dtype` and `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_da.dtype)\n",
    "print(a_da.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition, Dask arrays have additional attributes:\n",
    "\n",
    "- `npartitions` attribute tells you how many NumPy arrays make up the Dask array\n",
    "\n",
    "- `chunks` attribute which is a sequence of chunk sizes along each dimension of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_da.npartitions)\n",
    "print(a_da.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even visualize the Dask graph using the `visualize()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "a_da.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To compute the Dask graph for this array and load the data into memory, use the `compute()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask arrays supports most of the Numpy interface like the following:\n",
    "\n",
    "- Arithmetic and scalar mathematics, `+`, `*`, `exp`, `log`, ...\n",
    "\n",
    "- Reductions along axes, `sum()`, `mean()`, `std()`, `sum(axis=0)`, ...\n",
    "\n",
    "- Tensor contractions / dot products / matrix multiply, tensordot\n",
    "\n",
    "- Axis reordering / transpose, transpose\n",
    "\n",
    "- Slicing, `x[:100, 500:100:-2]`\n",
    "\n",
    "- Fancy indexing along single axes with lists or numpy arrays, `x[:, [10, 1, 5]]`\n",
    "\n",
    "- Array protocols like `__array__`, and `__array_ufunc__`\n",
    "\n",
    "- Some linear algebra `svd`, `qr`, `solve`, `solve_triangular`, `lstsq`, ...\n",
    "\n",
    "- ...\n",
    "\n",
    "See the [Dask array API docs](http://docs.dask.org/en/latest/array-api.html) for full details about what portion of the NumPy API is implemented for Dask arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, Dask leverages _blocked algorithms_ to perform computations that allow for the introduction of parallelism and a reduced RAM load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a_sum = a_da.sum()\n",
    "a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask supports a large portion of the NumPy API. This can be used to build up more complex computations using the familiar NumPy operations you're used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = da.ones((15, 15), chunks=(5, 5))\n",
    "y = (x + x.T).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dask DataFrames\n",
    "\n",
    "- Dask DataFrames are a collection of Pandas DataFrames\n",
    "\n",
    "- Dask DataFrames implement a subset of the Pandas API\n",
    "\n",
    "- For many purposes Dask DataFrames can serve as drop-in replacements for Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/dask-dataframe.svg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask DataFrames support many of the same data I/O methods as Pandas. For example, \n",
    "\n",
    "- `read_hdf` \\ `to_hdf`\n",
    "- `read_csv` \\ `to_csv`\n",
    "- `read_json` \\ `to_json`\n",
    "- `read_parquet` \\ `to_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_hdf('example_data.hdf', key='dataframe', chunksize=25)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Dask arrays, Dask DataFrames are lazily evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask DataFrames covers a well-used portion of the Pandas API. The following class of computations works well:\n",
    "\n",
    "- Elementwise operations: `df.x` + `df.y`, `df * df`\n",
    "\n",
    "- Row-wise selections: `df[df.x > 0]`\n",
    "\n",
    "- Loc: `df.loc[4.0:10.5]`\n",
    "\n",
    "- Common aggregations: `df.x.max()`, `df.max()`\n",
    "\n",
    "- Is in: `df[df.x.isin([1, 2, 3])]`\n",
    "\n",
    "- Datetime/string accessors: `df.timestamp.month`\n",
    "\n",
    "- Froupby-aggregate (with common aggregations): `df.groupby(df.x).y.max()`, `df.groupby('x').max()`\n",
    "\n",
    "- ...\n",
    "\n",
    "See the [Dask DataFrame API docs](http://docs.dask.org/en/latest/dataframe-api.html) for full details about what portion of the Pandas API is implemented for Dask DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum = ddf['col_1'].sum()\n",
    "a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Level Interface\n",
    "\n",
    "[ [Back to top](#Outline) ]\n",
    "\n",
    "Sometimes problems don’t fit into one of the high-level collections like Dask arrays or Dask DataFrames. In these cases, you can parallelize custom algorithms using the simpler Dask delayed interface. This allows one to create task graphs directly with a light annotation of normal python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct an example computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    return x + 2\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = sum(output)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask `delayed` wraps function calls. Wrapping a function in `delayed` will delay it's execution, instead returning a `Delayed` object that contains a graph of all operations done to get to the result. You can then call `compute` on a `Delayed` object to compute the task graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_add = delayed(add)(1, 1)\n",
    "simple_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_add.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `delayed` to make out previous example computation lazy by wrapping all the function calls with delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "@delayed\n",
    "def double(x):\n",
    "    return x + 2\n",
    "\n",
    "@delayed\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = delayed(sum)(output)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers\n",
    "\n",
    "[ [Back to top](#Outline) ]\n",
    "\n",
    "After Dask generates these task graphs it needs to execute them on parallel hardware. This is the job of a task scheduler. Different task schedulers exist. Each will consume a task graph and compute the same result, but with different performance characteristics. Dask has two families of task schedulers:\n",
    "\n",
    "- Single machine scheduler: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use. It can only be used on a single machine and does not scale.\n",
    "\n",
    "- Distributed scheduler: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Machine Schedulers\n",
    "\n",
    "- `'synchronous'`: The single-threaded synchronous scheduler executes all computations in the local thread, with no parallelism at all. This is particularly valuable for debugging and profiling, which are more difficult when using threads or processes.\n",
    "\n",
    "- `'threads'`: The threaded scheduler executes computations with a local `multiprocessing.pool.ThreadPool`. The threaded scheduler is the default choice for Dask arrays, Dask DataFrames, and Dask delayed. \n",
    "\n",
    "- `'processes'`: The multiprocessing scheduler executes computations with a local `multiprocessing.Pool`.\n",
    "\n",
    "- Distributed: Advanced distributed scheduler (Despite having \"distributed\" in it's name, the distributed scheduler is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can configure which scheduler is used is a few different ways. You can set the scheduler globablly by using the `dask.config.set(scheduler=)` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler='threads')\n",
    "total.compute(); # Will use the multi-threading scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use it as a context manager to set the scheduler for a block of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(scheduler='processes'):\n",
    "    total.compute()  # Will use the multi-processing scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even within a single compute call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.compute(scheduler='threads');  # Will use the multi-threading scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False, threads_per_worker=4,\n",
    "                n_workers=1, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.ones((20_000, 20_000), chunks=(2_000, 2_000))\n",
    "y = (x + x.T).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building on Dask: Dask-ML\n",
    "\n",
    "[ [Back to top](#Outline) ]\n",
    "\n",
    "[Dask-ML](http://ml.dask.org/) is a Python library for scalable machine learning in Python.\n",
    "\n",
    "Three different approaches are taken to scaling modern machine learning algorithms:\n",
    "\n",
    "- Parallelize scikit-learn directly\n",
    "\n",
    "- Reimplement scalable algorithms with Dask arrays\n",
    "\n",
    "- Partner with other distributed libraries (like XGBoost and TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.metrics import accuracy_score\n",
    "from dask_ml.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=2,\n",
    "                           n_classes=2,\n",
    "                           random_state=2,\n",
    "                           chunks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "[ [Back to top](#Outline) ]\n",
    "\n",
    "- Dask documentation: http://docs.dask.org\n",
    "\n",
    "- Dask examples repository: https://github.com/dask/dask-examples\n",
    "\n",
    "- There are lots of great Dask tutorial recordings from various Python conference on YouTube. For example,  \n",
    "\n",
    "    - SciPy 2018 (Dask): https://www.youtube.com/watch?v=mqdglv9GnM8\n",
    "    \n",
    "    - SciPy 2018 (Dask-ML): https://www.youtube.com/watch?v=ccfsbuqsjgI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
